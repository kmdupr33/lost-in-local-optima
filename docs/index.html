<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, minimum-scale=1.0"
    />
    <title>Lost In Local Optima</title>
    <meta property="og:title" content="Lost In Local Optima" />
    <meta charset="utf-8" />
    <meta property="og:type" content="article" />

    <meta property="og:description" content="How life-benefitting, habit-forming apps wrong users" />
    <meta property="description" content="How life-benefitting, habit-forming apps wrong users" />

    <link rel="stylesheet" href="static/idyll_styles.css" />
  </head>
  <body>
    <div id="idyll-mount"><div data-reactroot=""><div class="idyll-root"><div class=" idyll-text-container"></div><div class="article-header" style="background:#222222;color:#ffffff"><h1 class="hed">Lost In Local Optima</h1><h2 class="dek">How life-benefitting, habit-forming apps wrong users</h2><div class="byline">By: <a href="https://philosophicalhacker.com">Matt Dupree</a></div><div class="idyll-pub-date">Sat May 11 2019</div></div><div class=" idyll-text-container"><h2>Introduction</h2><p>We use apps that attempt to manipulate us.</p><p>LinkedIn’s profile strength indicator is a nice example of this.¹ It leverages the endowed progress effect to get people to fill out their profiles, and it looks like this:</p><img src="static/images/linkedin-profile.png" alt="linkedin-profile"/><p>When you first create your profile with no information, your profile strength is represented as a non-empty circle because some research² suggests that giving users a sense of a head-start on a task makes them more likely to complete that task, even if that head-start is artificial. In other words, if LinkedIn’s profile strength indicator started as an empty circle, we’d be less likely — all other things being equal — to add information to our profile.</p><p>Although manipulating people is prima facie immoral, some of us work on apps that use similar techniques to manipulate people. Serious attempts have been made to articulate a moral principle that justifies work on manipulative products, and most of them come down to the following principle:</p><blockquote>The better life principle: If my product makes my user’s life meaningfully better, then I’m justified in using product design techniques that manipulate them.</blockquote><p>The better life principle (BLP) is probably false, and most of us who are working on manipulative products are probably wronging our users.</p><p>The gist of my argument for this is that although some products may make users’ lives better, users may be able to find better ways of improving their lives, and it is wrong to deprive users of that opportunity by subverting their rational faculties with manipulative products. </p><p>This interactive essay is a clarification and defense of this argument.</p><h1>Argument, Objections, and Replies</h1></div><div class="fullWidth"><div class="argument"><div><p>
      Here’s the first premise of argument:</p><blockquote>(<!-- -->1<!-- -->) It’s wrong to deprive people the opportunity to find the best paths to happiness, even if we are pushing them down a path that makes their lives better.</blockquote><p>I think (1) is pretty intuitive. Check your intuitions on this thought experiment: Suppose I want a red car, but a car salesman manipulates me to choose a blue car instead of a red one for his gain. Although my life is materially improved (I have a car now), his actions are intuitively not morally kosher, right? I think the principle underlying this intuition is basically (1).</p><button>I accept (1)</button><button>I reject (1)</button></div><p><div style="display:none"><p>
      Here’s the second premise of the argument:</p><blockquote>(<!-- -->2<!-- -->) Manipulative product design often does this.</blockquote><p>I think (2) is also true. LinkedIn, for example, may make my life better, but are my current usage patterns ones that I’d find in the best (happiest) version of my life? Probably not.³</p><button>
        I accept (2)
      </button><button>
        I reject (2)
      </button></div><div style="display:none"><p>
      If you accept (1) and (2), you’ve got to accept (3):</p><blockquote>(<!-- -->3<!-- -->) So, manipulative product design is often wrong.</blockquote><p>This would mean that many of us who work on products that leverage manipulative design techniques are wronging our users. 
      
      If you don’t have any objections, go ahead and read the concluding thought.
      <button>Read Concluding Thought</button><button>I have an objection.</button></p></div>    <div style="display:none"><h1>Conclusion</h1><p>So, product designers probably shouldn’t manipulate their users. When we all build products that do this, we obstruct people’s attempts to optimize for happiness in their life. We push them down a path where they’re likely to become lost among local optima instead of preserving their opportunity to find their best life.
    </p></div><div style="display:none"><p>
      Here’s the argument in it’s entirety:</p><blockquote>(<!-- -->1<!-- -->) It’s wrong to deprive people the opportunity to find the best paths to happiness, even if we are pushing them down a path that makes their lives better.</blockquote><blockquote>(<!-- -->2<!-- -->) Manipulative product design often does this.</blockquote><blockquote>(<!-- -->3<!-- -->) So, manipulative product design is often wrong.</blockquote><p>If you don’t like (3), you have to reject (1) or (2).
      <button>Reject 1</button><button>Reject 2</button></p></div><div style="display:none"><h2>“This moral standard is too stringent to be true”</h2><p>Some may think (1) is too strong of a moral principle. They might think, “If (1) is true, most business transactions are morally wrong, and that can’t be right.” I’ve got two responses here.</p><p>First, (1) isn’t a strong as it appears. It only applies to people, not businesses. If a product designer uses manipulative product design techniques in B2B software, (1) wouldn’t apply. In that case, a company is at risk for being manipulated into a choice that results in a sub-optimal outcome, but this is often morally irrelevant.</p><p>Second, why should we balk at the idea that much of what happens in the business world is immoral? The rise of shareholder capitalism means that businesses are often explicitly not concerned with moral issues. Given the current political and social climate, I think the burden of proof is on those who believe that people are generally morally good in a business context. If someone can give us independent reasons to think that people are generally morally good in a businesss context, this could put some pressure on (1), but until then, I don’t see a reason for us to reject it.</p><button>
        I accept (1)
      </button><button>
        I’m still not convinced of (1)
      </button></div><div style="display:none"><h2>“Most ‘manipulative’ products don’t actually manipulate users in the way you’ve described”</h2><p>An easy objection here is that most of our products are not actually manipulative in the way I’ve described. This is basically a denial of (2).</p><p>You might look at the above LinkedIn example of the endowed progress effect and think leveraging the effect in product design isn’t manipulation. The reasoning for this would go something like this: If users are deciding not to fill out the profile merely because there doesn’t appear to be any progress towards that goal, this is actually an instance of irrational behavior. By using the endowed progress effect, according to this objection, we’re actually helping users overcome a fickle, irrational decision-making heuristic.</p><p>Here are two responses to this objection. First, don’t get hung up on the specific example. I’m not married to the idea that using the endowed progress effect in product design is manipulation. The bigger idea is that whenever manipulation occurs, it’s wrong, even if it’s used in a product that’s making a user’s life better. Second — and remember I don’t need this claim — I’d argue that leveraging the endowed progress effect in the LinkedIn case often involves deceiving a user into continuing to fill out their profile since they would have otherwise concluded that there wasn’t enough value in the product to justify the time investment to fill out their profile.⁴
      <button>
        I accept (2)
      </button><button>
        I’m still not convinced of (2)
      </button></p></div><div style="display:none">
      Well, looks like my argument won’t convince you. Have a nice day.
    </div>    </p></div></div><div class=" idyll-text-container"><em>This was interactive argument was made with <a href="https://idyll-lang.org/">Idyll</a></em><h1>Notes</h1><ol><li>Nir Eyal, <em>Hooked: How to build habit-forming products</em>, <!-- -->9<!-- -->0<!-- -->. The “endowed progress effect” is also discussed in the Heath Brother’s <em>Switch: How to change things when change is hard</em>.</li><li>Joseph C. Nunes and Xavier Dreze, “The Endowed Progress Effect: How Artificial Advancement Increases Effort.”</li><li>This is probably even more true with less professional social media apps like Instagram and Facebook.</li><li>Technically, this response doesn’t completely dispel the objection that most products aren’t manipulative in the problematic way of I’ve suggested here. It really only addresses some thinking around the particular LinkedIn example I’ve used. If someone presses me on this, we can talk more.</li></ol></div></div></div></div>
    <script src="static/idyll_index.js"></script>
  </body>
</html>
